{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_target = pd.read_csv('train_salaries.csv')\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "raw_merged = pd.merge(train_features, train_target, how='inner', on='jobId') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From data exploration we know the categorical and numerical variables\n",
    "categorical_vars = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "numerical_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_var = 'salary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    clean_df = df[(df['salary'] >= 8.5)] # Dropping outliers where 8.5 was the lowerbound based on inter-quartile range\n",
    "    return clean_df\n",
    "\n",
    "def get_encoded_features_df(df, cat_vars=None, num_vars=None):\n",
    "    ''' gets dummy variables for categorical variables and combines with numerical variables'''\n",
    "    category_df = pd.get_dummies(df[cat_vars])\n",
    "    numerical_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([category_df, numerical_df], axis=1)\n",
    "\n",
    "def get_target_df(df, target_var):\n",
    "    ''' returns target variable dataframe'''\n",
    "    return df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "clean_df = clean_data(raw_merged)\n",
    "train_features_df = get_encoded_features_df(clean_df, categorical_vars, numerical_vars)\n",
    "target_df = get_target_df(clean_df, target_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will run three different models:\n",
    "1) Linear Regression\n",
    "2) Random Forest\n",
    "3) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model list and error dictionaries\n",
    "models = []\n",
    "mean_mse={}\n",
    "cv_std={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to cross-validate models. Scoring metric is mean squared error\n",
    "\n",
    "def train_model(model, feature_df, target_df, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=3, scoring='neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0*np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)\n",
    "\n",
    "def print_summary(model, mean_mse, cv_std):\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse[model])\n",
    "    print('Standard deviation during CV:\\n', cv_std[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      " DummyRegressor(constant=None, quantile=None, strategy='mean')\n",
      "Average MSE:\n",
      " 1499.0193095475988\n",
      "Standard deviation during CV:\n",
      " 2.58357923724918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_reg = DummyRegressor(strategy='mean')\n",
    "train_model(dummy_reg, train_features_df, target_df, mean_mse, cv_std)\n",
    "print_summary(dummy_reg, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(n_estimators=150, max_depth=25, max_features=30) # Hyperparameters tuned by using GridSearchCV\n",
    "gbm = GradientBoostingRegressor(n_estimators =150, max_depth=5, loss='ls') # Hyperparameters tuned by using GridSearchCV\n",
    "\n",
    "models.extend([lr, rf, gbm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "Average MSE:\n",
      " 384.45647337937385\n",
      "Standard deviation during CV:\n",
      " 1.2864488234922833\n",
      "\n",
      "Model:\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=150, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 373.88899233176966\n",
      "Standard deviation during CV:\n",
      " 1.2442792275643169\n",
      "\n",
      "Model:\n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=150, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 356.86357964844433\n",
      "Standard deviation during CV:\n",
      " 0.9590687466068655\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    train_model(model, train_features_df, target_df, mean_mse, cv_std)\n",
    "    print_summary(model, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Gradient Boosting Regression model reduced the mean squared error to 356 compared to 1499 of the base model.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
